{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import math\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "#The following line has to run if you have not used nltk before\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre processing the text\n",
    "1.\tTo clean up the dataset I used the python csv reader and Pandas data frame. First, I read the csv file with the csv reader and made the airline sentiment and text into columns in the data frame. Before the program puts the data into the data frame it filters away everything but ascii characters with regex including punctuation. The function in the code used for this task is the pre_process_text function this function. The columns of the data frame in the train and test sets are tweets an classes, where the \"tweet\" column contains the tweets and the \"class\" column is the label the tweet has.\n",
    "\n",
    "3. I only used the airline_sentiment and text headers because I thought those were the best metadata headers to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_text_csv(data, encoding=\"utf-8\"):\n",
    "    #processing data and putting into a pandas dataframe\n",
    "    print(\"processing data\")\n",
    "    regex = re.compile('[^a-zA-Z ]')\n",
    "    processed_data = {\"tweet\":[], \"class\":[]}\n",
    "    line_count = 0\n",
    "    \n",
    "    with open(data, encoding = \"utf-8\") as csv_file:\n",
    "        #opening the csv\n",
    "        csv_reader = csv.reader(csv_file, delimiter=\",\")\n",
    "        classes = []\n",
    "        for row in csv_reader:\n",
    "            if line_count > 1:\n",
    "                if row[1] not in classes:\n",
    "                    classes.append(row[1])\n",
    "                    line_count +=1\n",
    "                    continue\n",
    "            #filtering the tweets with regex and removing stopwords\n",
    "            reviewsentence = remove_stopwords(regex.sub(\"\",row[10].encode('ascii', 'ignore').decode().lower()))\n",
    "            #Filtering the tweets without removing the stopwords will make the program run faster, \n",
    "            #but will make the accuracy more inconsistent\n",
    "            #reviewsentence = regex.sub(\"\",row[10].encode('ascii', 'ignore').decode().lower()))\n",
    "            #appending tweets and classes to the pandas dataframe\n",
    "            processed_data[\"tweet\"].append(reviewsentence)\n",
    "            processed_data[\"class\"].append(row[1])\n",
    "\n",
    "                \n",
    "            line_count +=1\n",
    "        \n",
    "        #converting the data into a pandas Data frame\n",
    "        pandas_processed_data = pd.DataFrame(processed_data)\n",
    "        #splitting the data randomly into a test and training set with np.random.rand\n",
    "        data_split = np.random.rand(len(pandas_processed_data)) < 0.8\n",
    "        train_set = pandas_processed_data[data_split]\n",
    "        test_set = pandas_processed_data[~data_split]\n",
    "        #returning the list of classes and the train and test sets\n",
    "\n",
    "    \n",
    "    return train_set, test_set, classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making the vocabulary\n",
    "The makevocab function takes a filtered tweet, goes through every word and puts the word into a set. the reason as to why I used a set is because a set is unindexed making it very fast to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vocab(\n",
    "    data):\n",
    "    \n",
    "    # Create an empty dictionary\n",
    "    allwords = set()\n",
    "    for tweet in data:\n",
    "         for word in tweet.split():\n",
    "            allwords.add(word)\n",
    "    return allwords\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The add_negation adds negation to words that ends with n't. I chose to not use this function because it did not really have an impact on the accuracy. I will have it in the code to show that it can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_negation(data):\n",
    "    #using regex to add negations to tweets\n",
    "    new_string = re.sub(r'(?:not|never|no|n\\'t)[\\w\\s]+[^\\w\\s]', \n",
    "  \t\tlambda match: re.sub(r'(\\s+)(\\w+)', r'\\1not_\\2', match.group(0)), \n",
    "\t\t\tdata,\n",
    "      flags=re.IGNORECASE)\n",
    "    \n",
    "    return new_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remove_stopwords function removes stopwords with nltk. The function loops through a tweet and filters out stopwords. Removing stopwords did not affect the accuracy to much, however the accuracy is much more consistent with the use of stopwords. \n",
    "The performance got affected, having to go through every tweet to remove stopwords had a big performance hit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(data):\n",
    "    #removing stopwords from tweets\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    filtered_sentence = []\n",
    "    for word in data.split():\n",
    "        #if a word is not in stop_words vocab and not \n",
    "        # in airlines the word will be removed from the tweets\n",
    "        if word not in stop_words:\n",
    "            filtered_sentence.append(word)\n",
    "\n",
    "    return \" \".join(filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The train_bayes function (Task 4, 5 and 6)\n",
    "\n",
    "This function takes the pandas datafram with the tweets and classes and finds the prior probaillities. The prior probabillities are found by finding the number of tweets in a single class and dividing it with the number of all tweets.\n",
    "The way it finds the prior probabillities is that it loops through the list with the 3 classes and for each class it accesses the data frame finding all tweets in each class\n",
    "\n",
    "The words_total_count is the amount of unique of words from all tweets in the data.\n",
    "\n",
    "\n",
    "To find the number of words in every class I used pandas to extract every single word and count all words in each class and the total amount of words for all tweets. The function returns the prior probabilities, the probabilities for all words and the vocabulary. The word probabilities are put in a dictionary where the word is a key and the probabilities for each class are the values.\n",
    "The comments in the code shows exactly which variables is what.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bayes(data, Classes):\n",
    "    #this function trains the classifier\n",
    "    print(\"training classifier\")\n",
    "    #using the make_vocab function make a vocabulary of words\n",
    "    vocab = make_vocab(data[\"tweet\"])\n",
    "    \n",
    "    all_docs = len(data[\"tweet\"])\n",
    "    all_word_likelihoods = {}\n",
    "    c_priors = {}\n",
    "    #looping for the three classes\n",
    "    for c in Classes:\n",
    "        #finding all tweet with a certain class\n",
    "        class_doc = data[data[\"class\"] == c]\n",
    "        #finding the prior probabilities of the classes\n",
    "        c_prior = len(class_doc)/all_docs\n",
    "        c_priors[c] = math.log(c_prior)\n",
    "        #finding how many times a word is used in the given class\n",
    "        word_freq_count = pd.Series(' '.join(class_doc[\"tweet\"]).split()).value_counts()\n",
    "        #finding the all the words used in all tweets in the given class\n",
    "        words_total_count = len(' '.join(class_doc[\"tweet\"].tolist()).split())\n",
    "        \n",
    "        for word in vocab:\n",
    "            #looping through the words in the vocabulary to find the probabillities of the words\n",
    "            if word not in all_word_likelihoods:\n",
    "                all_word_likelihoods[word] = {}\n",
    "            #if a word is not used in the given class the value is set to zero\n",
    "            if word not in word_freq_count:\n",
    "                word_freq = 0\n",
    "            else:\n",
    "                #finding the frequency of the given word\n",
    "                word_freq = word_freq_count[word]\n",
    "                \n",
    "            #finding the probabillity of the given word\n",
    "            word_likelihood = math.log((word_freq + 1)/(words_total_count + len(vocab)))\n",
    "            #adding the word to a dictionary where the word is the key and the class and probabillity is the values\n",
    "            all_word_likelihoods[word].update({c:word_likelihood})\n",
    "\n",
    "    return all_word_likelihoods, c_priors, vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The test_bayes function\n",
    "\n",
    "The classifier consists of the train_bayes and test_bayes function. First of all train_bayes function finds all the word frequencies for every class and the whole dataset. The probabilities for each word are calculated in a log space.\n",
    "The test_bayes function takes the list of classes, vocabulary, word probabilities and the prior probabilities as arguments. The function will then calculate the probability of a tweet for each class.\n",
    "\n",
    "First the function will loop through the list of classes, then it will loop through each tweet and word. Then the function will fetch each probability for each class for the given word and appends them to a list called tweet_class_prob.\n",
    "\n",
    "After the function has calculated the probabilities for every tweet, it adds the prior probabilities to the sum of the word probabilities for each class.\n",
    "The function returns a Pandas data frame with the columns tweet and class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_bayes(testdata, log_prior, \n",
    "                log_likelihood, Classes, Vocab):\n",
    "    #testing the classifier\n",
    "   \n",
    "    print(\"classifiying tweets\")\n",
    "    classified_tweets = {}\n",
    "    final_preds = {\"tweet\":[], \"class\":[]}\n",
    "    #looping through all classes\n",
    "    for c in Classes:\n",
    "        #finding the priors for the given class\n",
    "        sum_c = log_prior[c]\n",
    "        for tweet in testdata[\"tweet\"]:\n",
    "            if tweet not in classified_tweets:\n",
    "                classified_tweets[tweet] = {}\n",
    "            tweet_class_prob = []\n",
    "            for word in tweet.split():\n",
    "                if word in Vocab:\n",
    "                    #putting the probabilities of the words into the a tweet_class_prob list\n",
    "                    log_likelihood[word][c]\n",
    "                    tweet_class_prob.append(log_likelihood[word][c])\n",
    "            #adding the prior and the sum tweet_class_prob list of the words for the given class\n",
    "            classified_tweets[tweet].update({c:sum_c + np.sum(tweet_class_prob)})\n",
    "            \n",
    "\n",
    "    for tweet in testdata[\"tweet\"]:\n",
    "        #finds the class with the highest probability for every tweet and lables the tweets accordingly\n",
    "        final_preds[\"tweet\"].append(tweet)\n",
    "        final_preds[\"class\"].append(Classes[np.argmax([classified_tweets[tweet][\"positive\"],classified_tweets[tweet][\"neutral\"], classified_tweets[tweet][\"negative\"]])])\n",
    "        \n",
    "    return pd.DataFrame(final_preds)\n",
    "    [\"positive\", \"negative\", \"neutral\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy score\n",
    "\n",
    "The evaluation of the classifier is done with the score function. It simply takes the test set and the set with the new labels and checks the number of labels that are equal and calculates the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score(label, target):\n",
    "        #finding the accuracy score of the classifier by comparing the test set\n",
    "        #and the data returned from the test_bayes function\n",
    "        compare = []\n",
    "        #compares the labels in the test set with the new labels\n",
    "        for i in range(0,len(label)):\n",
    "            if label.iloc[i] == target.iloc[i]:\n",
    "                temp ='correct'\n",
    "                compare.append(temp)\n",
    "            else:\n",
    "                temp ='incorrect'\n",
    "                compare.append(temp)\n",
    "        comparison = Counter(compare)\n",
    "        accuracy = comparison['correct']/(comparison['correct']+comparison['incorrect'])\n",
    "        return f\"accuracy score: {accuracy * 100}%\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The take_tweet function\n",
    "\n",
    "This function does pretty much the same as the test_bayes function, but modified for taking a single tweet instead of a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_tweet(C,vocab, \n",
    "               log_prior, log_likelihood, user_inp):\n",
    "    #taking a tweet and classifying it \n",
    "    regex = re.compile('[^a-zA-Z ]')\n",
    "    filtered_input = regex.sub(\"\",user_inp.lower())\n",
    "    word_probs = {}\n",
    "    class_probs = []\n",
    "    #the same process from the test bayes function is used here, \n",
    "    #except it has benn modified for one tweet\n",
    "    for c in C:\n",
    "        sum_c = log_prior[c]\n",
    "        word_class_prob = []\n",
    "        for word in filtered_input.split():\n",
    "            if word not in word_probs:\n",
    "                word_probs[word] = []\n",
    "            if word in vocab:\n",
    "                word_class_prob.append(log_likelihood[word][c])\n",
    "                if (c,log_likelihood[word][c]) not in word_probs[word]:\n",
    "                    word_probs[word].append((c,log_likelihood[word][c]))\n",
    "            else:\n",
    "                continue\n",
    "        class_probs.append(sum_c + np.sum(word_class_prob))\n",
    "        word_class_prob = []\n",
    "            \n",
    "            \n",
    "    #returns the label with the highest probabillity, and word_probabillities\n",
    "    return C[np.argmax(class_probs)], word_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Explanation Generator\n",
    "\n",
    "The explanation generator fetches all the word probabillities for every class, as well as the prior probabillities. The function uses this data to print out the probabbilities of each word. After the generator has printed out every probabillity it shows the sums of all the classes and returns the explanation for the given label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explanation_generator(word_probs, log_prior, vocab):\n",
    "    #returns an explanation of why a tweet has been labeled with a class\n",
    "    classes = [\"positive\", \"neutral\", \"negative\"]\n",
    "    pos_prob = []\n",
    "    neg_prob = []\n",
    "    neu_prob = []\n",
    "    #this loop prints the word probabilities for all classes\n",
    "    for word in word_probs.keys():\n",
    "        #finding the probabillities\n",
    "        if word in vocab:\n",
    "            print(\"P(\" \n",
    "            + word \n",
    "            + \"|positive) = \" \n",
    "            + str(word_probs[word][0][1])\n",
    "            + \" | P(\" \n",
    "            + word \n",
    "            + \"|neutral) = \" \n",
    "            + str(word_probs[word][1][1])\n",
    "            + \" | P(\" \n",
    "            + word \n",
    "            + \"|negative) = \" \n",
    "            + str(word_probs[word][2][1])\n",
    "            + \"\\n\")\n",
    "            pos_prob.append(word_probs[word][0][1])\n",
    "            neu_prob.append(word_probs[word][1][1])\n",
    "            neg_prob.append(word_probs[word][2][1])\n",
    "\n",
    "    #summing all the probabillities to show the probabillities of all classes\n",
    "    print(\"P(positive) + P(tweet|positive) = \" \n",
    "        + str(log_prior[\"positive\"]) \n",
    "        + \" + \" \n",
    "        + str(sum(pos_prob)) \n",
    "        + \" = \" \n",
    "        , log_prior[\"positive\"] \n",
    "        + sum(pos_prob),\"\\n\")\n",
    "    print(\"P(negative) + P(tweet|negative) = \" \n",
    "        + str(log_prior[\"negative\"]) \n",
    "        + \" + \" \n",
    "        + str(sum(neg_prob)) \n",
    "        + \" = \"\n",
    "        , log_prior[\"negative\"] \n",
    "        + sum(neg_prob),\"\\n\")\n",
    "    print(\"P(neutral) + P(tweet|neutral) = \" \n",
    "        + str(log_prior[\"neutral\"]) \n",
    "        + \" + \" \n",
    "        + str(sum(neu_prob))\n",
    "        + \" = \"\n",
    "        , log_prior[\"neutral\"] \n",
    "        +sum(neu_prob), \"\\n\")\n",
    "        \n",
    "    all_probs = [log_prior[\"positive\"] \n",
    "                 + sum(pos_prob),log_prior[\"neutral\"] \n",
    "                 + sum(neu_prob), log_prior[\"negative\"] \n",
    "                 + sum(neg_prob)]\n",
    "    #returns the highest probabillity for the tweet\n",
    "    return \"this tweet is labeled \" + classes[np.argmax(all_probs)] +\" because the probabillity is highest for the \" + classes[np.argmax(all_probs)] + \" label.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The accuracy score and explanations for task 11\n",
    "The accuracy score will vary for each run of the program because the data is randomly distributed in the test and training sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data\n",
      "training classifier\n",
      "classifiying tweets\n",
      "accuracy score: 76.5482233502538%\n"
     ]
    }
   ],
   "source": [
    "train_set, test_set, classes = pre_process_text_csv(\"Tweets.csv\")\n",
    "    \n",
    "\n",
    "log_likelihood, log_priors, vocab = train_bayes(train_set, classes)\n",
    "\n",
    "final_preds = test_bayes(test_set,log_priors,log_likelihood,classes,vocab)\n",
    "\n",
    "print(accuracy_score(test_set[\"class\"],final_preds[\"class\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanations\n",
    "In the cells under you can see the explanations for the tweets for task 11.\n",
    "The explanations generator prints out every word in the tweet and shows the probabilities for each class.\n",
    "after printing out all probailities the explanation generator sums up the probabilities from all the words and adds the prior probabilities which is (Tweet|class).\n",
    "\n",
    "## Tweet 1\n",
    "@VirginAmerica is anyone doing anything there today?  Website is useless and no one is answering the phone\n",
    "\n",
    "This tweet is orginally labeled as negative in the dataset. The classifier gave it the negative label because most of the words had a Higher probability of being negative. The only word that did not have a higher probability of being negative was \"today\".\n",
    "## Tweet 2\n",
    "@VirginAmerica is the best airline I have flown on.Easy to change your reservation,helpful representatives a comfortable flying experience\n",
    "\n",
    "This tweet was originally label positive in the dataset, and was labeled as positive by the classifier. Most of the words in this tweet had the higher probability of being in a positive tweet.\n",
    "## Tweet 3\n",
    "@VirginAmerica Can you find us a flt out of LAX that is sooner than midnight on Monday? That would be great customer service ðŸ˜ƒ. \n",
    "\n",
    "This tweet was originally labeled as neutral but got labeled as negative by the classifier Most of the words in this tweet had a higher probabillity of being in a negative tweet with only some words having a higher probaility of being negative. The high prior probabillity of being negative probably had a big impact.\n",
    "## Tweet 4\n",
    "@VirginAmerica  Flight Booking Problems last second flight for next week from SFO- to SAN any chance you want to gift me a promo code since I love you guys\n",
    "\n",
    "This tweet was originally labeled as positive, but was labeled neutral by the classifier. Most of the words have a higher probability of being neutral than negative.\n",
    "\n",
    "all probabilities for every word is shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label given by classifier: negative\n",
      "\n",
      "P(virginamerica|positive) = -5.513393682436653 | P(virginamerica|neutral) = -5.449949787678571 | P(virginamerica|negative) = -6.404972493523188\n",
      "\n",
      "P(anyone|positive) = -8.466566341571847 | P(anyone|neutral) = -8.45986669622452 | P(anyone|negative) = -7.041678363178183\n",
      "\n",
      "P(anything|positive) = -8.31241566174459 | P(anything|neutral) = -7.314734391921516 | P(anything|negative) = -7.0916887837528435\n",
      "\n",
      "P(today|positive) = -6.131191425754811 | P(today|neutral) = -6.345333834733413 | P(today|negative) = -6.170282951211918\n",
      "\n",
      "P(website|positive) = -8.872031449680012 | P(website|neutral) = -7.572563501223616 | P(website|negative) = -6.729898739147341\n",
      "\n",
      "P(useless|positive) = -10.258325810799903 | P(useless|neutral) = -10.405776845279833 | P(useless|negative) = -8.35735515708412\n",
      "\n",
      "P(one|positive) = -6.544753744095595 | P(one|neutral) = -6.311432283057732 | P(one|negative) = -5.610667163276496\n",
      "\n",
      "P(answering|positive) = -9.565178630239958 | P(answering|neutral) = -10.405776845279833 | P(answering|negative) = -8.190301072420953\n",
      "\n",
      "P(phone|positive) = -7.2138033730764795 | P(phone|neutral) = -7.004579463617677 | P(phone|negative) = -5.806490539504322\n",
      "\n",
      "P(positive) + P(tweet|positive) = -1.8313166041612967 + -70.87766011939983 =  -72.70897672356114 \n",
      "\n",
      "P(negative) + P(tweet|negative) = -0.4649025380563768 + -61.403335263099365 =  -61.86823780115574 \n",
      "\n",
      "P(neutral) + P(tweet|neutral) = -1.553457661226229 + -69.27001364901672 =  -70.82347131024295 \n",
      "\n",
      "this tweet is labeled negative because the probabillity is highest for the negative label.\n"
     ]
    }
   ],
   "source": [
    "#explanation for correctly labled tweet\n",
    "    \n",
    "tweet_label, word_probs = take_tweet(\n",
    "classes, vocab, log_priors, \n",
    "log_likelihood,\n",
    "\"@VirginAmerica is anyone doing anything there today?  Website is useless and no one is answering the phone.\")\n",
    "print(\"label given by classifier: \" + tweet_label + \"\\n\")\n",
    "\n",
    "print(explanation_generator(word_probs, log_priors, vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label given by classifier: positive\n",
      "\n",
      "P(virginamerica|positive) = -5.513393682436653 | P(virginamerica|neutral) = -5.449949787678571 | P(virginamerica|negative) = -6.404972493523188\n",
      "\n",
      "P(best|positive) = -5.815674554309586 | P(best|neutral) = -7.461337866113392 | P(best|negative) = -7.459413563878161\n",
      "\n",
      "P(airline|positive) = -6.068671068773477 | P(airline|neutral) = -7.270282629350683 | P(airline|negative) = -6.01031830151924\n",
      "\n",
      "P(flown|positive) = -8.466566341571847 | P(flown|neutral) = -8.45986669622452 | P(flown|negative) = -8.55802585254627\n",
      "\n",
      "P(oneasy|positive) = -9.565178630239958 | P(oneasy|neutral) = -10.405776845279833 | P(oneasy|negative) = -11.448397610442436\n",
      "\n",
      "P(change|positive) = -7.485737088560121 | P(change|neutral) = -6.143096968238517 | P(change|negative) = -6.457965023663699\n",
      "\n",
      "P(reservationhelpful|positive) = -9.565178630239958 | P(reservationhelpful|neutral) = -10.405776845279833 | P(reservationhelpful|negative) = -11.448397610442436\n",
      "\n",
      "P(representatives|positive) = -9.565178630239958 | P(representatives|neutral) = -10.405776845279833 | P(representatives|negative) = -10.75525042988249\n",
      "\n",
      "P(comfortable|positive) = -8.178884269120067 | P(comfortable|neutral) = -9.712629664719888 | P(comfortable|negative) = -11.448397610442436\n",
      "\n",
      "P(flying|positive) = -6.250992625567432 | P(flying|neutral) = -6.345333834733413 | P(flying|negative) = -6.4785843108664345\n",
      "\n",
      "P(experience|positive) = -6.962488944795573 | P(experience|neutral) = -9.019482484159942 | P(experience|negative) = -6.660905867660389\n",
      "\n",
      "P(positive) + P(tweet|positive) = -1.8313166041612967 + -83.43794446585461 =  -85.26926107001592 \n",
      "\n",
      "P(negative) + P(tweet|negative) = -0.4649025380563768 + -93.13062867486718 =  -93.59553121292356 \n",
      "\n",
      "P(neutral) + P(tweet|neutral) = -1.553457661226229 + -91.0793104670584 =  -92.63276812828464 \n",
      "\n",
      "this tweet is labeled positive because the probabillity is highest for the positive label.\n"
     ]
    }
   ],
   "source": [
    "#explanation for the second correctly labled tweet\n",
    "tweet_label, word_probs = take_tweet(\n",
    "classes, vocab, log_priors, \n",
    "log_likelihood,\n",
    "\"@VirginAmerica is the best airline I have flown on.Easy to change your reservation,helpful representatives a comfortable flying experience\")\n",
    "print(\"label given by classifier: \" + tweet_label + \"\\n\")\n",
    "print(explanation_generator(word_probs, log_priors, vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label given by classifier: negative\n",
      "\n",
      "P(virginamerica|positive) = -5.513393682436653 | P(virginamerica|neutral) = -5.449949787678571 | P(virginamerica|negative) = -6.404972493523188\n",
      "\n",
      "P(find|positive) = -8.178884269120067 | P(find|neutral) = -7.461337866113392 | P(find|negative) = -6.884049418974599\n",
      "\n",
      "P(us|positive) = -5.876299176126021 | P(us|neutral) = -6.049068018590241 | P(us|negative) = -5.401025431396158\n",
      "\n",
      "P(flt|positive) = -7.619268481184644 | P(flt|neutral) = -7.840827487818296 | P(flt|negative) = -6.863430131771864\n",
      "\n",
      "P(lax|positive) = -7.550275609697692 | P(lax|neutral) = -7.147680307258351 | P(lax|negative) = -7.354053048220335\n",
      "\n",
      "P(sooner|positive) = -8.872031449680012 | P(sooner|neutral) = -9.712629664719888 | P(sooner|negative) = -9.65663814121438\n",
      "\n",
      "P(midnight|positive) = -10.258325810799903 | P(midnight|neutral) = -8.796338932845732 | P(midnight|negative) = -9.050502337644065\n",
      "\n",
      "P(monday|positive) = -8.872031449680012 | P(monday|neutral) = -7.840827487818296 | P(monday|negative) = -8.04720022878028\n",
      "\n",
      "P(would|positive) = -6.3463028053717565 | P(would|neutral) = -5.790656328438573 | P(would|negative) = -5.768225001425368\n",
      "\n",
      "P(great|positive) = -5.043390053190917 | P(great|neutral) = -7.840827487818296 | P(great|negative) = -7.18571773340112\n",
      "\n",
      "P(customer|positive) = -5.643205293958643 | P(customer|neutral) = -7.227723014931887 | P(customer|negative) = -5.274611506540499\n",
      "\n",
      "P(service|positive) = -5.39851340643823 | P(service|neutral) = -6.555629243569774 | P(service|negative) = -5.059836204896806\n",
      "\n",
      "P(positive) + P(tweet|positive) = -1.8313166041612967 + -85.17192148768454 =  -87.00323809184584 \n",
      "\n",
      "P(negative) + P(tweet|negative) = -0.4649025380563768 + -82.95026167778866 =  -83.41516421584504 \n",
      "\n",
      "P(neutral) + P(tweet|neutral) = -1.553457661226229 + -87.71349562760128 =  -89.26695328882751 \n",
      "\n",
      "this tweet is labeled negative because the probabillity is highest for the negative label.\n"
     ]
    }
   ],
   "source": [
    "#explanation for wrongly labeled tweet\n",
    "#this tweet is orginally classified as neutral\n",
    "tweet_label, word_probs = take_tweet(\n",
    "    classes, vocab, \n",
    "    log_priors, log_likelihood,\n",
    "    \"@VirginAmerica Can you find us a flt out of LAX that is sooner than midnight on Monday? That would be great customer service ðŸ˜ƒ\")\n",
    "print(\"label given by classifier: \" + tweet_label + \"\\n\")\n",
    "print(explanation_generator(word_probs, log_priors,vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label given by classifier: neutral\n",
      "\n",
      "P(virginamerica|positive) = -5.513393682436653 | P(virginamerica|neutral) = -5.449949787678571 | P(virginamerica|negative) = -6.404972493523188\n",
      "\n",
      "P(flight|positive) = -4.612878913156665 | P(flight|neutral) = -4.261591211154187 | P(flight|negative) = -3.7046943522686817\n",
      "\n",
      "P(booking|positive) = -7.693376453338366 | P(booking|neutral) = -7.038481015293359 | P(booking|negative) = -7.157938169294044\n",
      "\n",
      "P(problems|positive) = -7.773419161011902 | P(problems|neutral) = -7.004579463617677 | P(problems|negative) = -6.686223675644679\n",
      "\n",
      "P(last|positive) = -7.00022927277842 | P(last|neutral) = -6.879416320663671 | P(last|negative) = -6.222650936729234\n",
      "\n",
      "P(second|positive) = -8.31241566174459 | P(second|neutral) = -8.007881572481462 | P(second|negative) = -8.270343780094489\n",
      "\n",
      "P(next|positive) = -6.891029980813428 | P(next|neutral) = -6.7168973911658965 | P(next|negative) = -6.694807419336071\n",
      "\n",
      "P(week|positive) = -7.550275609697692 | P(week|neutral) = -7.766719515664573 | P(week|negative) = -7.337523746269124\n",
      "\n",
      "P(sfo|positive) = -7.550275609697692 | P(sfo|neutral) = -7.410044571725842 | P(sfo|negative) = -7.497153891861008\n",
      "\n",
      "P(san|positive) = -7.955740717805857 | P(san|neutral) = -7.515405087383668 | P(san|negative) = -8.014410405957289\n",
      "\n",
      "P(chance|positive) = -8.872031449680012 | P(chance|neutral) = -6.940040942480106 | P(chance|negative) = -8.116193100267232\n",
      "\n",
      "P(want|positive) = -7.773419161011902 | P(want|neutral) = -6.668107226996464 | P(want|negative) = -6.342452136541855\n",
      "\n",
      "P(gift|positive) = -9.565178630239958 | P(gift|neutral) = -9.712629664719888 | P(gift|negative) = -10.062103249322545\n",
      "\n",
      "P(promo|positive) = -9.159713522131792 | P(promo|neutral) = -8.326335303599997 | P(promo|negative) = -10.75525042988249\n",
      "\n",
      "P(code|positive) = -9.159713522131792 | P(code|neutral) = -8.007881572481462 | P(code|negative) = -8.675808888202655\n",
      "\n",
      "P(since|positive) = -7.860430538001532 | P(since|neutral) = -8.007881572481462 | P(since|negative) = -6.686223675644679\n",
      "\n",
      "P(love|positive) = -5.653155624811811 | P(love|neutral) = -6.940040942480106 | P(love|negative) = -7.556577312331809\n",
      "\n",
      "P(guys|positive) = -5.758516140469637 | P(guys|neutral) = -6.692204778575524 | P(guys|negative) = -6.24439092336564\n",
      "\n",
      "P(positive) + P(tweet|positive) = -1.8313166041612967 + -134.6551936509597 =  -136.48651025512098 \n",
      "\n",
      "P(negative) + P(tweet|negative) = -0.4649025380563768 + -132.42971858653672 =  -132.8946211245931 \n",
      "\n",
      "P(neutral) + P(tweet|neutral) = -1.553457661226229 + -129.3460879406439 =  -130.89954560187013 \n",
      "\n",
      "this tweet is labeled neutral because the probabillity is highest for the neutral label.\n"
     ]
    }
   ],
   "source": [
    "#second wrongly labeled tweet\n",
    "#originally labeled positive\n",
    "tweet_label, word_probs = take_tweet(\n",
    "    classes, vocab, \n",
    "    log_priors, log_likelihood,\n",
    "    \"@VirginAmerica  Flight Booking Problems last second flight for next week from SFO- to SAN any chance you want to gift me a promo code since I love you guys\")\n",
    "print(\"label given by classifier: \" + tweet_label + \"\\n\")\n",
    "print(explanation_generator(word_probs, log_priors,vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
