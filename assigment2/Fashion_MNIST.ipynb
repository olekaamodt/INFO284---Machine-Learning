{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Fashion_MNIST.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RgxPw42Xw917"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Pyg2hRTHw919"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FMXhl-jNmgmd",
        "outputId": "36756a90-47bc-430f-e702-387ceb23cdea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        }
      },
      "source": [
        "!pip3 install torch==1.4.0+cu92 torchvision==0.5.0+cu92 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.4.0+cu92\n",
            "  Downloading https://download.pytorch.org/whl/cu92/torch-1.4.0%2Bcu92-cp38-cp38-win_amd64.whl (641.9 MB)\n",
            "Collecting torchvision==0.5.0+cu92\n",
            "  Downloading https://download.pytorch.org/whl/cu92/torchvision-0.5.0%2Bcu92-cp38-cp38-win_amd64.whl (1.2 MB)\n",
            "Requirement already satisfied: six in c:\\users\\jorge\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from torchvision==0.5.0+cu92) (1.14.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\jorge\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from torchvision==0.5.0+cu92) (1.18.1)\n",
            "Collecting pillow>=4.1.1\n",
            "  Downloading Pillow-7.0.0-cp38-cp38-win_amd64.whl (2.0 MB)\n",
            "Installing collected packages: torch, pillow, torchvision\n",
            "Successfully installed pillow-7.0.0 torch-1.4.0+cu92 torchvision-0.5.0+cu92\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sOHtbOwH0aU0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a0a48105-6628-456a-b1d6-a927ee6ae282"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import sklearn.metrics as metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from itertools import chain "
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ERROR! Session/line number was not unique in database. History logging moved to new session 63\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NXSw-0WW4xya",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 500\n",
        "\n",
        "## transformations\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor()]) \n",
        "\n",
        "train_set = torchvision.datasets.FashionMNIST(\"./data\", download=True, transform=\n",
        "                                                transforms.Compose([transforms.ToTensor()]))\n",
        "test_set = torchvision.datasets.FashionMNIST(\"./data\", download=True, train=False, transform=\n",
        "                                               transforms.Compose([transforms.ToTensor()]))  \n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(train_set, \n",
        "                                           batch_size=BATCH_SIZE)\n",
        "testloader = torch.utils.data.DataLoader(test_set,\n",
        "                                          batch_size=BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fZ66AF5Bw91_"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YTu5oOnj5RH5",
        "colab": {}
      },
      "source": [
        "class FashionCNN(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(FashionCNN, self).__init__()\n",
        "        \n",
        "        self.conv_layer1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=1, out_channels=50, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(50),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        \n",
        "        self.conv_layer2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=50, out_channels=100, kernel_size=3),\n",
        "            nn.BatchNorm2d(100),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        \n",
        "        self.conv_layer3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=100, out_channels=200, kernel_size=3),\n",
        "            nn.BatchNorm2d(200),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "\n",
        "        self.fc1 = nn.Linear(in_features=200*2*2, out_features = 600)\n",
        "        self.drop = nn.Dropout2d(0.5)\n",
        "        self.fc2 = nn.Linear(in_features=600, out_features=120)\n",
        "        self.fc3 = nn.Linear(in_features=120, out_features=10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.conv_layer1(x)\n",
        "        out = self.conv_layer2(out)\n",
        "        out = self.conv_layer3(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc1(out)\n",
        "        self.drop(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.fc3(out)\n",
        "        \n",
        "        return F.log_softmax(out, dim=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mTa5iPe952Ju",
        "outputId": "fde493c9-baf3-417e-e1ff-121af14fbe4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "## test the model with 1 batch\n",
        "model = FashionCNN()\n",
        "for images, labels in trainloader:\n",
        "    print(\"batch size:\", images.shape)\n",
        "    out = model(images)\n",
        "    print(out.shape)\n",
        "    break"
      ],
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "batch size: torch.Size([500, 1, 28, 28])\n",
            "torch.Size([500, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "L4OC-8-Jw91-"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jiaGnDrl6AN2",
        "colab": {}
      },
      "source": [
        "learning_rate = 0.002\n",
        "num_epochs = 6\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = FashionCNN()\n",
        "model = model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "## compute accuracy\n",
        "def get_accuracy(logit, target, batch_size):\n",
        "    ''' Obtain accuracy for training round '''\n",
        "    corrects = (torch.max(logit, 1)[1].view(target.size()).data == target.data).sum()\n",
        "    accuracy = 100.0 * corrects/batch_size\n",
        "    return accuracy.item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gPlkCKek6ktf",
        "outputId": "e474886b-b49c-48ce-f187-051c42424dd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "for epoch in range(num_epochs):\n",
        "    train_running_loss = 0\n",
        "    train_acc = 0\n",
        "\n",
        "    model = model.train()\n",
        "\n",
        "    ## training step\n",
        "    for i, (images, labels) in enumerate(trainloader):\n",
        "        \n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        ## forward + backprop + loss\n",
        "        logits = model(images)\n",
        "        loss = criterion(logits, labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        ## update model params\n",
        "        optimizer.step()\n",
        "\n",
        "        train_running_loss += loss.detach().item()\n",
        "        train_acc += get_accuracy(logits, labels, BATCH_SIZE)\n",
        "    \n",
        "    model.eval()\n",
        "    print('Epoch: %d | Loss: %.4f | Train Accuracy: %.2f' \\\n",
        "          %(epoch, train_running_loss / (i + 1), train_acc/(i+1)))"
      ],
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 | Loss: 0.5242 | Train Accuracy: 80.96\n",
            "Epoch: 1 | Loss: 0.2952 | Train Accuracy: 89.33\n",
            "Epoch: 2 | Loss: 0.2524 | Train Accuracy: 90.73\n",
            "Epoch: 3 | Loss: 0.2164 | Train Accuracy: 92.05\n",
            "Epoch: 4 | Loss: 0.1897 | Train Accuracy: 93.07\n",
            "Epoch: 5 | Loss: 0.1744 | Train Accuracy: 93.73\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "c6ZHXl7k6zQe",
        "outputId": "8d00a5cb-cfb4-4ec3-ccd3-4f3c6f3394ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_acc = 0.0\n",
        "\n",
        "correct_labels = []\n",
        "predicted_labels = []\n",
        "for i, (images, labels) in enumerate(testloader, 0):\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "    #returns a tensor array of the correct labels in the test set\n",
        "    correct_labels.append(labels)\n",
        "\n",
        "    outputs = model(images)\n",
        "    predictions = torch.max(outputs, 1)[1].to(device)\n",
        "    predicted_labels.append(predictions)\n",
        "    #returns a tensor array of the predicted labels of the test set\n",
        "\n",
        "    test_acc += get_accuracy(outputs, labels, BATCH_SIZE)\n",
        "        \n",
        "print('Test Accuracy: %.2f'%( test_acc/(i + 1)))\n"
      ],
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy: 90.48\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Af2A6VCaw92X",
        "outputId": "f01554a6-39a4-4ea6-e6aa-f888c035ced8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "#confusion matrix\n",
        "#converting the tensor array of the correct labels to a regular list\n",
        "predictions_l = [predicted_labels[i].tolist() for i in range(len(predicted_labels))]\n",
        "#converting the tensor array of the predicted labels to a regular list\n",
        "labels_l = [correct_labels[i].tolist() for i in range(len(correct_labels))]\n",
        "#print(labels_l)\n",
        "#chain.from_iterable makes it so lists that have [[x1,x2,x3],[y1,y2,y3]] = [x1,x2,x3,y1,y2,y3]\n",
        "#Basicly it makes a list of lists into one big list.\n",
        "predictions_l = list(chain.from_iterable(predictions_l))\n",
        "labels_l = list(chain.from_iterable(labels_l))\n",
        "#print(labels_l)\n",
        "\n",
        "#calculates the confusion matrix with the lists of predicted and correct labels\n",
        "print(\"accuracy score \", metrics.accuracy_score(labels_l, predictions_l))\n",
        "confusion_matrix(labels_l, predictions_l)\n",
        "\n",
        "print(\"Classification report for CNN :\\n%s\\n\"\n",
        "      % (metrics.classification_report(labels_l, predictions_l)))"
      ],
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy score  0.9048\n",
            "Classification report for CNN :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.86      0.86      1000\n",
            "           1       0.98      0.99      0.98      1000\n",
            "           2       0.86      0.89      0.87      1000\n",
            "           3       0.85      0.94      0.89      1000\n",
            "           4       0.78      0.91      0.84      1000\n",
            "           5       0.95      0.99      0.97      1000\n",
            "           6       0.87      0.57      0.69      1000\n",
            "           7       0.98      0.94      0.96      1000\n",
            "           8       0.99      0.98      0.98      1000\n",
            "           9       0.97      0.97      0.97      1000\n",
            "\n",
            "    accuracy                           0.90     10000\n",
            "   macro avg       0.91      0.90      0.90     10000\n",
            "weighted avg       0.91      0.90      0.90     10000\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "G8h1bqSLDLSt",
        "outputId": "a7f38d9e-c788-42ca-81e9-51b5e5dbb33a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "model_save_name = 'classifier.pt'\n",
        "path = F\"/content/gdrive/My Drive/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kBP1-ClQ-T3n",
        "outputId": "9f0edc59-e590-4525-a2a3-2195dd518ed7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "\n",
        "\n",
        "# Print model's state_dict\n",
        "print(\"Model's state_dict:\")\n",
        "for param_tensor in model.state_dict():\n",
        "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
        "\n"
      ],
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model's state_dict:\n",
            "conv_layer1.0.weight \t torch.Size([50, 1, 3, 3])\n",
            "conv_layer1.0.bias \t torch.Size([50])\n",
            "conv_layer1.1.weight \t torch.Size([50])\n",
            "conv_layer1.1.bias \t torch.Size([50])\n",
            "conv_layer1.1.running_mean \t torch.Size([50])\n",
            "conv_layer1.1.running_var \t torch.Size([50])\n",
            "conv_layer1.1.num_batches_tracked \t torch.Size([])\n",
            "conv_layer2.0.weight \t torch.Size([100, 50, 3, 3])\n",
            "conv_layer2.0.bias \t torch.Size([100])\n",
            "conv_layer2.1.weight \t torch.Size([100])\n",
            "conv_layer2.1.bias \t torch.Size([100])\n",
            "conv_layer2.1.running_mean \t torch.Size([100])\n",
            "conv_layer2.1.running_var \t torch.Size([100])\n",
            "conv_layer2.1.num_batches_tracked \t torch.Size([])\n",
            "conv_layer3.0.weight \t torch.Size([200, 100, 3, 3])\n",
            "conv_layer3.0.bias \t torch.Size([200])\n",
            "conv_layer3.1.weight \t torch.Size([200])\n",
            "conv_layer3.1.bias \t torch.Size([200])\n",
            "conv_layer3.1.running_mean \t torch.Size([200])\n",
            "conv_layer3.1.running_var \t torch.Size([200])\n",
            "conv_layer3.1.num_batches_tracked \t torch.Size([])\n",
            "fc1.weight \t torch.Size([600, 800])\n",
            "fc1.bias \t torch.Size([600])\n",
            "fc2.weight \t torch.Size([120, 600])\n",
            "fc2.bias \t torch.Size([120])\n",
            "fc3.weight \t torch.Size([10, 120])\n",
            "fc3.bias \t torch.Size([10])\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}